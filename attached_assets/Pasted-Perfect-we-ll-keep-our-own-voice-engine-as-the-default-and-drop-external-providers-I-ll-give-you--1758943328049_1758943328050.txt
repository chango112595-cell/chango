Perfect — we’ll keep our own voice engine as the default and drop external providers. I’ll give you copy-paste files that plug straight into your current Replit build (the one-file bootstrap you’re running). This delivers:
	•	Chango Voice Engine (CVE): phoneme+prosody planner, accent & emotion shaping, pacing, emphasis
	•	Client playback via browser (WebSpeech) using our plan (no ElevenLabs/Azure)
	•	Optional plug-in points left in place (disabled) so you can add vendors later without refactoring

⸻

1) Server: Chango Voice Engine

server/voice/engine.js

// Chango Voice Engine (CVE) — phoneme+prosody planner with accent & emotion shaping
// Output: { text, plan, prosody } for client playback (WebSpeech) or future local TTS.

const VOWELS = /[aeiouyAEIOUY]/;
const PUNCT = /([,;:!?])/g;
const WORD_SPLIT = /\s+/;

const BASE_PROSODY = {
  rate: 1.0,       // speech rate (1.0 = neutral)
  pitch: 1.0,      // pitch multiplier
  volume: 1.0,     // loudness
  pauseComma: 180, // ms
  pausePeriod: 280,
  pauseClause: 160
};

const EMOTIONS = {
  neutral:  { rate: 1.0, pitch: 1.0, volume: 1.0, pitchVar: .02, rateVar: .03 },
  calm:     { rate: 0.95,pitch: 0.98,volume: 0.95,pitchVar: .01, rateVar: .02 },
  cheerful: { rate: 1.05,pitch: 1.06,volume: 1.05,pitchVar: .03, rateVar: .04 },
  serious:  { rate: 0.97,pitch: 0.94,volume: 0.98,pitchVar: .01, rateVar: .02 },
  empathetic:{rate:0.98,pitch: 1.02,volume: 1.02,pitchVar: .02, rateVar: .02 }
};

const ACCENTS = {
  neutral:   { name: 'Neutral',    transform: (w,i)=>w },
  brit_rp:   { name: 'British RP', transform: (w,i)=> w.replace(/([aeiouAEIOU])r\b/g,(m,v)=>v) },
  southern_us:{name:'Southern US', transform: (w,i)=> i>.4 ? w.replace(/\byou all\b/ig,'y’all') : w },
  spanish_en:{name:'Spanish-EN',   transform: (w,i)=> {
    let x=w; if(i>.3) x=x.replace(/\bvery\b/ig,'bery');
    if(i>.5) x=x.replace(/th/g,'d').replace(/TH/g,'D');
    return x;
  }},
  caribbean: { name:'Caribbean',   transform: (w,i)=> i>.35 ? w.replace(/th/g,'t').replace(/TH/g,'T') : w }
};

function rand(){ return Math.random(); }
function jitter(v,a){ return +(v + (rand()*2-1)*a).toFixed(3); }

function tokenize(text){
  // split but keep punctuation tokens
  const out=[]; let buf='';
  for(const ch of text){
    if(',.;:!?'.includes(ch)){ if(buf) out.push(buf), buf=''; out.push(ch); }
    else if(/\s/.test(ch)){ if(buf) out.push(buf), buf=''; }
    else buf+=ch;
  }
  if(buf) out.push(buf);
  return out;
}

function phonemize(word){
  // ultra-light heuristic phonemizer (placeholder for future ML)
  const w = word.toLowerCase();
  const syl = Math.max(1, (w.match(VOWELS)||[]).length);
  return { syl };
}

function planProsody(tokens, opts){
  const { accent='neutral', intensity=0.5, emotion='neutral' } = opts||{};
  const emo = EMOTIONS[emotion]||EMOTIONS.neutral;
  const base = { ...BASE_PROSODY };
  const a = ACCENTS[accent]||ACCENTS.neutral;

  const plan=[]; let outWords=[];
  for(let i=0;i<tokens.length;i++){
    const t = tokens[i];
    if(',;:'.includes(t)){ plan.push({ type:'pause', ms: base.pauseComma }); continue; }
    if('!?'.includes(t)){ plan.push({ type:'pause', ms: base.pauseClause+60 }); continue; }
    if('.'.includes(t)){ plan.push({ type:'pause', ms: base.pausePeriod }); continue; }
    const wAcc = a.transform(t, intensity);
    const ph = phonemize(wAcc);
    // syllable-based micro-timing
    const rate = jitter(base.rate*emo.rate*(1 + (ph.syl-1)*0.02), emo.rateVar);
    const pitch= jitter(base.pitch*emo.pitch, emo.pitchVar);
    const vol  = base.volume*emo.volume;
    plan.push({ type:'word', w:wAcc, rate, pitch, volume: vol });
    outWords.push(wAcc);
    // gentle micro pause between long words
    if(wAcc.length>10 && rand()<0.25) plan.push({ type:'pause', ms: 60 });
  }

  // smooth pauses: merge neighbors, cap extremes
  for(let i=1;i<plan.length;i++){
    if(plan[i-1].type==='pause' && plan[i].type==='pause'){
      plan[i-1].ms = Math.min(600, plan[i-1].ms + plan[i].ms);
      plan.splice(i,1); i--;
    }
  }

  const prosody = {
    engine: 'CVE-1',
    route: 'client',
    emotion,
    accent,
    intensity,
    base
  };
  return { text: outWords.join(' '), plan, prosody };
}

module.exports = { planProsody, ACCENTS, EMOTIONS };

server/routes/voice.js

const { Router } = require('express');
const { planProsody } = require('../voice/engine');
const r = Router();

// POST /voice/plan  { text, accent, intensity, emotion }
r.post('/voice/plan', (req,res)=>{
  try{
    const { text='', accent='neutral', intensity=0.5, emotion='neutral' } = req.body||{};
    if(!text || typeof text!=='string') return res.status(400).json({ ok:false, error:'text required' });
    const plan = planProsody(text, { accent, intensity: +intensity, emotion });
    return res.json({ ok:true, ...plan });
  }catch(e){ return res.status(500).json({ ok:false, error:String(e.message||e) }); }
});

// keep a simple GET for quick tests
r.get('/voice/ping', (_q,res)=> res.json({ ok:true, engine:'CVE-1', route:'client' }));

module.exports = r;

Mount it (add one line)

Edit server/index.js and add:

app.use('/', require('./routes/voice'));


⸻

2) Client hookup (uses our plan; no vendors)

Patch client/index.html (small additions)
	•	Under Accent Emulator card controls, add emotion + voice picker:

<select id="emotionSel">
  <option value="neutral">Neutral</option>
  <option value="calm">Calm</option>
  <option value="cheerful">Cheerful</option>
  <option value="serious">Serious</option>
  <option value="empathetic">Empathetic</option>
</select>
<select id="selVoice"></select>

Replace speak() in client/app.js

async function speak(text){
  lastUtteranceRaw=text;
  const accent=(document.getElementById('accentProfile')||{}).value||'neutral';
  const intensity=parseFloat((document.getElementById('accentIntensity')||{value:'0.5'}).value||'0.5');
  const emotion=(document.getElementById('emotionSel')||{}).value||'neutral';

  // ask server for a prosody plan
  let plan;
  try{
    const res = await fetch('/voice/plan',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({text,accent,intensity,emotion})});
    const js = await res.json(); if(!js.ok) throw new Error(js.error||'plan error'); plan = js.plan; lastUtteranceSaid = js.text;
  }catch(e){ // fallback to local simple accent if server unavailable
    const styled = applyAccent(text);
    lastUtteranceSaid = styled.text;
    return speakClient(styled.text, styled);
  }

  // render plan via WebSpeech (client)
  speechSynthesis.cancel();
  let seq = Promise.resolve();
  for(const step of plan){
    if(step.type==='pause'){
      seq = seq.then(()=> new Promise(r=> setTimeout(r, step.ms)));
    } else if(step.type==='word'){
      seq = seq.then(()=> new Promise(r=>{
        const u = new SpeechSynthesisUtterance(step.w);
        const v = pickVoice(); if(v) u.voice=v;
        u.rate = Math.max(.5, Math.min(2, step.rate||1));
        u.pitch= Math.max(.5, Math.min(2, step.pitch||1));
        u.volume=Math.max(0, Math.min(1, step.volume??1));
        u.onstart=()=>status('speaking…'); u.onend=()=>r(); u.onerror=()=>r();
        speechSynthesis.speak(u);
      }));
    }
  }
  seq.then(()=>status('idle'))
     .catch(()=>status('idle'));
  // count utterance (diagnostics)
  fetch('/diagnostics/incr',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({key:'ttsClientUtterances'})}).catch(()=>{});
}

This keeps all voice logic ours: server creates the plan; client speaks it. ElevenLabs/Azure are not used or required.

⸻

3) Remove vendors (optional but clean)
	•	Delete any ElevenLabs/Azure files/routes.
	•	Ensure no ELEVENLABS_API_KEY / AZURE_* env vars are referenced.
	•	In diagnostics, keep routes shown as disabled (helps you toggle later if you ever want).

⸻

4) Quick test

# server plan
curl -s -X POST $REPLIT_URL/voice/plan \
 -H 'content-type: application/json' \
 -d '{"text":"Hello, I am Chango.","accent":"brit_rp","intensity":0.6,"emotion":"cheerful"}' | jq .

In the web UI:
	1.	Enable Voice
	2.	Type text → Speak
	3.	Switch Accent/Intensity/Emotion and speak again

⸻

What you’ve got now
	•	A self-contained voice program (CVE) driven by our own phoneme+prosody planning.
	•	Human-like rhythm via pauses, micro-timing, syllable pacing, emotion shaping.
	•	Accent emulation without third-party APIs.
	•	Clean extension points for future local neural TTS (if/when we add a WASM vocoder).

If you want, next I can add a client “Export Audio” button (records WebSpeech output to WAV in-browser and sends it to /exports), fully offline and still vendor-free.